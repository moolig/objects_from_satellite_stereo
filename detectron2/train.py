# -*- coding: utf-8 -*-
"""Detectron2 Tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5


# Detectron2 Beginner's Tutorial

<img src="https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png" width="500">

Welcome to detectron2! In this tutorial, we will go through some basics usage of detectron2, including the following:
* Run inference on images or videos, with an existing detectron2 model
* Train a detectron2 model on a new dataset

You can make a copy of this tutorial or use "File -> Open in playground mode" to play with it yourself.

# Install detectron2
"""



from detectron2.utils.logger import setup_logger
setup_logger()

import cv2
import random
import os

import numpy as np
import json
from detectron2.structures import BoxMode

from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog
from detectron2.utils.visualizer import ColorMode
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.data import build_detection_test_loader
from detectron2.data.datasets import register_coco_instances
from detectron2.data.datasets import coco
from detectron2.engine import DefaultTrainer
from datetime import datetime
date_time_now = datetime.now()
day_now_str = date_time_now.strftime("%d_%b_%Y")




# img_path = r'/home/shmuelgr/Desktop/temp/image/test.jpg'
# im = cv2.imread(img_path)
# cv2.imshow(im)
#
# """Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image."""
#
# cfg = get_cfg()
# # add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library
# cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
# # Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well
# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
# predictor = DefaultPredictor(cfg)
# outputs = predictor(im)
#
# # look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification
# outputs["instances"].pred_classes
# outputs["instances"].pred_boxes
#
# # We can use `Visualizer` to draw the predictions on the image.
# v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
# v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
# cv2.imshow(v.get_image()[:, :, ::-1])
#
# """# Train on a custom dataset
#
# In this section, we show how to train an existing detectron2 model on a custom dataset in a new format.
#
# We use [the balloon segmentation dataset](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon)
# which only has one class: balloon.
# We'll train a balloon segmentation model from an existing model pre-trained on COCO dataset, available in detectron2's model zoo.
#
# Note that COCO dataset does not have the "balloon" category. We'll be able to recognize this new class in a few minutes.
#
# ## Prepare the dataset
# """



if __name__ == "__main__":

    """Register the balloon dataset to detectron2, following the [detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).
    Here, the dataset is in its custom format, therefore we write a function to parse it and prepare it into detectron2's standard format. See the tutorial for more details.
    """

    model_train_name = 'airs_dataset_train'
    model_val_name = 'airs_dataset_val'
    # model_train_name = "balloon_train"
    # model_val_name = "balloon_val"


    root_air = r'/media/shmuelgr/676f1b15-b0a9-4795-945e-d2a40ccdc70f/Dataset/AIRS//'

    root_air = r'/media/shmuelgr/676f1b15-b0a9-4795-945e-d2a40ccdc70f/Dataset/smallAerialImageDataset//'

    train_image_root = root_air + r'/train/images'
    train_json_file = root_air + r'/train/coco_format.json'
    val_image_root = root_air + r'/val/images'
    val_json_file = root_air + r'/val/coco_format.json'



    register_coco_instances(model_train_name, {}, train_json_file, train_image_root)
    register_coco_instances("airs_dataset_val", {}, val_json_file, val_image_root)


    # root_data = r'/media/shmuelgr/676f1b15-b0a9-4795-945e-d2a40ccdc70f/Dataset/balloon/'
    def get_balloon_dicts(img_dir):
        json_file = os.path.join(img_dir, "via_region_data.json")
        with open(json_file) as f:
            imgs_anns = json.load(f)

        dataset_dicts = []
        for idx, v in enumerate(imgs_anns.values()):
            record = {}

            filename = os.path.join(img_dir, v["filename"])
            height, width = cv2.imread(filename).shape[:2]

            record["file_name"] = filename
            record["image_id"] = idx
            record["height"] = height
            record["width"] = width

            annos = v["regions"]
            objs = []
            for _, anno in annos.items():
                assert not anno["region_attributes"]
                anno = anno["shape_attributes"]
                px = anno["all_points_x"]
                py = anno["all_points_y"]
                poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]
                poly = [p for x in poly for p in x]

                obj = {
                    "bbox": [np.min(px), np.min(py), np.max(px), np.max(py)],
                    "bbox_mode": BoxMode.XYXY_ABS,
                    "segmentation": [poly],
                    "category_id": 0,
                    "iscrowd": 0
                }
                objs.append(obj)
            record["annotations"] = objs
            dataset_dicts.append(record)
        return dataset_dicts

    # from detectron2.data import DatasetCatalog, MetadataCatalog
    # for d in ["train", "val"]:
    #     DatasetCatalog.register("balloon_" + d, lambda d=d: get_balloon_dicts(root_data + d))
    #     MetadataCatalog.get("balloon_" + d).set(thing_classes=["balloon"])
    #
    #
    metadata = MetadataCatalog.get(model_train_name)

    """To verify the data loading is correct, let's visualize the annotations of randomly selected samples in the training set:"""
    # dataset_dicts = get_balloon_dicts(root_data + "/train")

    airs_metadata = MetadataCatalog.get("airs_dataset_train")
    # dataset_dicts = coco.load_coco_json(train_json_file, train_image_root, 'airs_dataset_train')

    # for d in random.sample(dataset_dicts, 3):
    #     img = cv2.imread(d["file_name"])
    #     # visualizer = Visualizer(img[:, :, ::-1], metadata=balloon_metadata, scale=0.5)
    #     visualizer = Visualizer(img[:, :, ::-1], metadata=airs_metadata, scale=0.5)
    #     vis = visualizer.draw_dataset_dict(d)
    #     res_im = vis.get_image()[:, :, ::-1]
    #     print('start ima!!!!!!!!!!!!1')
    #     cv2.imshow('image', res_im)
    #     cv2.waitKey(0)
    #     cv2.destroyAllWindows()

    """## Train!
    
    Now, let's fine-tune a coco-pretrained R50-FPN Mask R-CNN model on the balloon dataset. It takes ~6 minutes to train 300 iterations on Colab's K80 GPU, or ~2 minutes on a P100 GPU.
    """



    cfg = get_cfg()
    cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
    cfg.DATASETS.TRAIN = (model_train_name,)
    cfg.DATASETS.TEST = ()
    cfg.DATALOADER.NUM_WORKERS = 2
    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")  # Let training initialize from model zoo
    cfg.SOLVER.IMS_PER_BATCH = 2
    cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR
    cfg.SOLVER.MAX_ITER = 30000   # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset
    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)
    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class

    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
    trainer = DefaultTrainer(cfg)
    trainer.resume_or_load(resume=False)
    trainer.train()

    # Commented out IPython magic to ensure Python compatibility.
    # Look at training curves in tensorboard:
    # %load_ext tensorboard
    # %tensorboard --logdir output

    """## Inference & evaluation using the trained model
    Now, let's run inference with the trained model on the balloon validation dataset. First, let's create a predictor using the model we just trained:
    """

    cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model
    cfg.DATASETS.TEST = (model_val_name, )
    predictor = DefaultPredictor(cfg)

    """Then, we randomly select several samples to visualize the prediction results."""


    # dataset_dicts = get_balloon_dicts(root_data + "/val")
    dataset_dicts = coco.load_coco_json(val_json_file, val_image_root, 'airs_dataset_val')
    for d in random.sample(dataset_dicts, 3):
        im = cv2.imread(d["file_name"])
        outputs = predictor(im)
        v = Visualizer(im[:, :, ::-1],
                       metadata=metadata,
                       scale=0.8,
                       instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels
        )
        v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
        res_im = v.get_image()[:, :, ::-1]
        cv2.imshow('image', res_im)
        cv2.waitKey(0)
        cv2.destroyAllWindows()


    """We can also evaluate its performance using AP metric implemented in COCO API.
    This gives an AP of ~70%. Not bad!
    """
    evaluator = COCOEvaluator(model_val_name, cfg, False, output_dir="./output/" + day_now_str + '/')
    val_loader = build_detection_test_loader(cfg, model_val_name)

    inference_on_dataset(trainer.model, val_loader, evaluator)
    # another equivalent way is to use trainer.test

    """# Other types of builtin models"""

    # # Inference with a keypoint detection model
    # cfg = get_cfg()
    # cfg.merge_from_file(model_zoo.get_config_file("COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml"))
    # cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model
    # cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml")
    # predictor = DefaultPredictor(cfg)
    # outputs = predictor(im)
    # v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
    # v = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    #
    # res_im = v.get_image()[:, :, ::-1]
    # cv2.imshow('image', res_im)
    # cv2.waitKey(0)
    # cv2.destroyAllWindows()

    # # Inference with a panoptic segmentation model
    # cfg = get_cfg()
    # cfg.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
    # cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
    # predictor = DefaultPredictor(cfg)
    # panoptic_seg, segments_info = predictor(im)["panoptic_seg"]
    # v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
    # v = v.draw_panoptic_seg_predictions(panoptic_seg.to("cpu"), segments_info)
    #
    # res_im = v.get_image()[:, :, ::-1]
    # cv2.imshow('image', res_im)
    # cv2.waitKey(0)
    # cv2.destroyAllWindows()



